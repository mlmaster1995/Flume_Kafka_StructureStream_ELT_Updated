#Copyright 2021 C.Young
#
#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#distributed under the License is distributed on an "AS IS" BASIS,
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#See the License for the specific language governing permissions and
#limitations under the License.

####To use any of the following pipeline, uncomment and define the pipeline.source, pipeline.sink and app properties
#vmstat => flume => kafka producer => spark structured stream => console/ hdfs/ hive table/ kafka producer/ mongoDB/ mySQL
#tweet_stream => kafka producer => spark structured stream => console/ hdfs/ hive table/ kafka producer/ mongoDB/ mySQL
#covid19_batch_data => kafka producer => spark structured stream => console/ hdfs/ kafka producer

####Sources:
#1. "vmstat source" is manged by "Flume". check and run "start-vmstats-with-flume.sh" to start to source
#depends on the property "kafka.topic.vmstat", "kafka.topic.brokers"
#pipeline.source=vmstat

#2. "tweet stream" source is managed by "TwitterStreamToKafkaProducer" app. check and run "start-tweetStream-to-kafkaProducer.sh"
#depends on the property "kafka.topic.tweet", "kafka.topic.brokers"
#pipeline.source=tweet

#3. "covid19 batch data" source is managed by the Apache Airflow DAG. check and run "covid19_data_pipeline.py" in Apache Airflow
#depends on the property "kafka.topic.covid19", "kafka.topic.brokers"
#pipeline.source=covid

####Sinks:
#1. "console" sink depends on the property "console.write.mode"
#pipeline.sink=console

#2. "kafka" sink depends on properties "kafka.topic.kafkaProducer"(the kafka sink topic), "kafka.topic.vmstat" (the kafka souce topic), "kafka.brokers"
#pipeline.sink=kafka

#3. "mysql" sink depends on all mysql properties
#pipeline.sink=mysql

#4. "mongoDB" sink depends on properties "mongo.InputURI", "mongoOutputURI" and both must be set up in the spark session
#pipeline.sink=mongoDB

#5. "hdfs" sink depends on all hdfs properties
#pipeline.sink=hdfs

#6. "hiveTable" sink depends on all hive properties
#pipeline.sink=hiveTable

### Application Props
# spark session properties
# if mongoDB is selected in the sink, both properteis "mongo.InputURI", "mongo.OutputURI"must be set in this file. Spark needs the mongo properties to set up the session.
#sparkSession.mode=local
#sparkSession.app.name=elt_pipeline

# mongoDB properties
#mongo.InputURI=mongodb://127.0.0.1/test.fromstream
#mongo.OutputURI=mongodb://127.0.0.1/test.fromstream

# kafka producer properties
#kafka.brokers=localhost:9101
#kafka.topic.vmstat=exec
#kafka.topic.kafkaProducer=toKafka
#kafka.topic.tweet=tweet
#kafka.topic.covid19=covidSummary

# console write properties
#console.write.mode=append

# mysql properties
#mysql.url=jdbc:mysql://localhost:3306
#mysql.driver=com.mysql.cj.jdbc.Driver
#mysql.username=...
#mysql.password=...
#mysql.database=...
#mysql.table.vmstat=...
#mysql.table.tweet=...
#mysql.write.mode=...

# hdfs properties
#hdfs.path.vmstat=hdfs://localhost:9000/user/ky/data/stream_data_vmstat/
#hdfs.path.tweet=hdfs://localhost:9000/user/ky/data/stream_data_tweet/
#hdfs.path.covid19=hdfs://localhost:9000/user/ky/data/stream_data_covid19/
#hdfs.checkpoint.path.vmstat=hdfs://localhost:9000/user/ky/checkpoint/vmstat/
#hdfs.checkpoint.path.tweet=hdfs://localhost:9000/user/ky/checkpoint/tweet/
#hdfs.checkpoint.path.covid19=hdfs://localhost:9000/user/ky/checkpoint/covid19/
#hdfs.write.format=parquet
#hdfs.write.mode=append
#hdfs.compression.type=snappy

# hive properties
#hive.write.format=parquet
#hive.write.mode=append
#hive.compression.type=snappy
#hive.metastore.database=chrisy
#hive.metastore.table.vmstat=fromStream
#hive.metastore.table.tweet=fromTweet
#hive.partitions.number=1


