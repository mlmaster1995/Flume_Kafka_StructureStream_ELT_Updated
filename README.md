## ETL(Extract Transform Load) Data Pipeline Collection OOP
![Hex.pm](https://img.shields.io/hexpm/l/plug?logo=Apache&logoColor=%23ff0000&style=flat-square)

### Table of Contents
* [About the Project](#about-the-project)
* [What's New](#what's-new)
* [Pipelines](#pipelines)  
  * [Data Srouces](#data-sources)
  * [Pipeline Structure](#pipeline-structure)
  * [Pipeline List](#pipeline-list)
  * [Use Pipeline](#use-pipeline)  
* [Built With](#built-with)
* [Project Content](#project-content)
* [Structure Data Samples](#structure-data-samples)
* [Contact](#contact)

### About The Project
This project is updated from [Stream Data Pipeline with Flume Kafka StructureStream](https://github.com/mlmaster1995/Flume_Kafka_StructureStream_ELT) 
, but this work is developed with newer version of Apache Kafka, Spark, Flume, Airflow. The whole project is developed and tested in the centOS7 VM configured with all technologies
as [Built With](#built-with).

### What's New
* Newer version of Apache Kafka and Spark in Scala 2.12;
* Add twitter stream source;
* Add COVID-19 batch data source with Apache Airflow;   
* Use Apache Kafka Producer API in Scala;
* Use Apache Kafka Consumer API in Scala;   
* Add Avro schema to the tweet stream source with Confluent Schema Registry;  
* Add more sinks including mySQL, HiveTable, MongoDB;
* Upgrade to an Object-Oriented Project;

### Pipelines
#### Data Sources: 
1. Real-time system information stream is generated by the linux command ```$ vmstate 1``` which is setup in the flume agent configuration file ```vmstat_flume_kafka.conf```. 
2. Real-time twitter stream is generated by calling twitter4j api, and the stream is redirected into the kafka producer which could be configured by three different modes (```forget-and-fire, sync, async```) 
and also put avro schema into the tweet data. All these configuration and setup are in the application folder ```TwitterStreamToKafkaProducer```.
3. Batch COVID-19 data is generated by calling [CCODWG api](https://opencovid.ca/about/) and scheduled by Apache airflow. The data is redirected into the basic kafka producer and the DAG is 
defined in the folder ```Covid19ToKafkaProducer```
    
#### Pipeline Structure:

![Big Data Flow Charts](https://user-images.githubusercontent.com/55723894/109090676-5aeaf980-76e1-11eb-856a-40bc4ccdff49.jpeg)

#### Pipeline List:


    |    Sources          |                  Pipelines                            |                               Sinks                          |
    | ------------------- | ----------------------------------------------------- | ------------------------------------------------------------ |
    |    vmstat           |   flume => kafka producer => spark structured stream  |   console/ hdfs/ hive table/ kafka producer/ mongoDB/ mySQL  |
    | tweet stream        |   kafka producer => spark structured stream           |   console/ hdfs/ hive table/ kafka producer/ mongoDB/ mySQL  |
    | tweet stream        |   kafka producer + Schema Registry                    |   confluent kafka-avro-consumer/ kafka consumer              |
    | covid19 batch data  |   kafka producer => spark structured stream           |   console/ hdfs/ kafka producer                              |
    | covid19 batch data  |   kafka producer                                      |   kafka consumer                                             |

#### Use Pipeline:
1. "vmstat stream" is managed by Flume. Check and run ```./start-vmstats-with-flume.sh``` to stream the data and ingest into kafka producer.
   
2. "twitter stream" is managed by TwitterStreamToKafkaProducer app. To set up Twitter api credentials or modify kafka producer props, check ```ApplicationProperties.scala``` in the main folder
in the application folder and then run ```sbt assembly``` to generate the fat jar file ```Twitter_Stream_Source-assembly-0.1.jar```. Finally run ```./start-tweetStream-to-kafkaProducer.sh``` in 
the terminal to stream the data and ingest into kafka producer.

3. "covid19 batch data" is managed by Apache Airflow. Copy ```covid19_data_pipeline.py``` in the Covid19ToKafkaProducer to the dags folder and config ```http connection``` in the airflow webUI and
then trigger the pipeline to collect data and ingest into kafka producer.

4. "kafka consumer sink" is managed by KafkaConsumer app. To modify kafka consumer props, check ```ApplicationProperties.scala``` in the main folder in the application folder and run. 
```sbt assembly``` to generate the fat jar file ```KafkaConsumer-assembly-0.1.jar``` and then run ```./start-kafkaConsumer.sh``` in the terminal to start the consumer.

5. "spark data pipeline" is managed by the property file ```kafka-spark-unit.properties``` and it has all properties for all sinks (console, hdfs, hive table, kafka producer, mongoDB, mySQL) and 
options to select which pipeline to run. All jars are in the ```jars folder``` including dependencies. After config the ```kafka-spark-unit.properties``` file, 
run ```./start-spark-kafka-unit.sh ./kafka-spark-unit.properties``` to in the terminal to start the pipeline.

### Built With
* [Scala 2.12.10](https://www.scala-lang.org/download/2.12.10.html)
* [Python 3.8.x](https://www.python.org/) 
* [Apache Spark 3.0.1](https://spark.apache.org/docs/2.1.1/)
* [Apache Flume 1.9.0](https://flume.apache.org/releases/1.5.2.html)
* [Apache Kafka 2.7.0](https://kafka.apache.org/0102/documentation.html)
* [Apache Airflow 2.0.0](https://airflow.apache.org/)  
* [Apache Hadoop 2.7.7](https://hadoop.apache.org/)
* [Apache Hive 2.3.8](https://hive.apache.org/)
* [Confluent Schema Registry (Community Platform 6.1.0)](https://github.com/confluentinc/schema-registry)  
* [Twitter4j 4.0.7](http://twitter4j.org/en/index.html)
* [MongoDB 4.2](https://www.mongodb.com/)
* [MySQL 8.0.x](https://www.mysql.com/)

### Project Content

    ├── Covid19ToKafkaProducer                    # Airflow DAG for COVID-19 batch data
    ├── KafkaConsumer                             # KakfaConsumer application for tweet stream and COVID-19 data sources
    ├── KafkaSparkUnit                            # Spark extract, transform and load application 
    ├── TwitterStreamToKafkaProducer              # Twitter Stream to Kafka application
    ├── create-database-table.sql                 # SQL script for create the database and tables in mySQL
    ├── start-kafkaConsumer.sh                    # Bash script to run scala application KafkaConsumer
    ├── start-spark-kafka-unit.sh                 # Bash script to submit spark application KafkaSparkUnit
    ├── start-tweetStream-to-kafkaProducer.sh     # Bash script to run scala application KafkaConsumer
    ├── start-vmstats-with-flume.sh               # Bash script to run flume for vmstat data stream
    ├── vmstat_flume_kafka.conf                   # Flume agent configuration file
    ├── jars                                      # Jar folder for KafkaSparkUnit app including all dependencies
    ├── kafka-spark-unit.properties               # Properties for data pipeline and the pipeline selection

### Structure Data Samples
**NOTE**: Sensitive Data Is Hidden Or Modified In The Following Samples. 

* Pipeline: vmstat -> flume -> kafka -> spark structured streaming -> mySQL

 
    | row_id | topic | time                | r    | b    | swpd | free   | buff | cache   | si   | so   | bi   | bo   | in_val | cs   | us   | sy   | id   | wa   | st   |
    |--------|-------|---------------------|------|------|------|--------|------|---------|------|------|------|------|--------|------|------|------|------|------|------|
    |      1 | exec  | 2021-02-02 10:43:02 | 1    | 2    | 3    | 4      | 5    | 6       | 7    | 8    | 9    | 10   | 11     | 12   | 13   | 14   | 15   | 16   | 17   |
    |      2 | exec  | 2021-02-02 10:56:47 | 0    | 0    | 8    | 301620 | 1144 | 8950572 | 0    | 0    | 0    | 35   | 1706   | 1672 | 6    | 2    | 92   | 0    | 0    |
    |      3 | exec  | 2021-02-02 10:56:47 | 0    | 0    | 8    | 301176 | 1144 | 8950576 | 0    | 0    | 0    | 0    | 1469   | 1540 | 4    | 2    | 95   | 0    | 0    |
    |      4 | exec  | 2021-02-02 10:56:47 | 1    | 0    | 8    | 247564 | 1144 | 8950612 | 0    | 0    | 0    | 0    | 3564   | 3661 | 15   | 4    | 81   | 0    | 0    |
    |      5 | exec  | 2021-02-02 10:56:50 | 2    | 0    | 8    | 170608 | 1144 | 8919396 | 0    | 0    | 0    | 0    | 5363   | 4051 | 35   | 5    | 60   | 0    | 0    |
   

* Pipeline: tweet stream -> kafka -> spark structured streaming -> mySQL


    | row_id | tweet_time                   | user_id  | full_name           | tweet_id  | tweet_source        | is_truncated | is_rt | tweet_text                         |
    |--------|------------------------------|----------|---------------------|-----------|---------------------|--------------|-------|------------------------------------|
    |      1 | Fri Feb 12 20:04:55 EST 2021 |   ...    |      ...            |   ...     | Twitter for iPhone  | false        | false | just ordered ... 🥰 ...       ...  |
    |      2 | Fri Feb 12 20:04:55 EST 2021 |   ...    | chrisy 🌼@pptyaacy  |   ...     | Twitter for Android | false        | false | @bluexjjkyu okeyyy,           ...  |
    |      3 | Fri Feb 12 20:04:55 EST 2021 |   ...    |      ...            |   ...     | Twitter for iPhone  | false        | false | RT @uhprome: I really         ...  |
    |      4 | Fri Feb 12 20:04:55 EST 2021 |   ...    |      ...            |  ...      | Twitter for iPhone  | false        | false | RT @thesecret: Every          ...  |
    |      5 | Fri Feb 12 20:04:55 EST 2021 |   ...    |      ...            |   ...     | Twitter for iPhone  | false        | false | RT @ferbIatin: the            ...  |

* Pipeline: tweet stream -> kafka -> spark structred streaming -> mongoDB
  

      {
        "_id" : ObjectId("60271b6f6a142c2014fdc296"),
        "tweet_time" : "Fri Feb 12 19:20:53 EST 2021",
        "user_id" : "...",
        "full_name" : "...",
        "tweet_id" : "...",
        "tweet_source" : "Twitter for iPhone",
        "is_truncated" : "false",
        "is_rt" : "false",
        "tweet_text" : "First Time She Put Dat Pussy On Me I Put Her In A Benz 🤞🏽"
      }

* Pipeline: tweet stream -> kafka + Schema Registry -> Confluent Kafka Avro Consumer 
    
     
     {"tweetdate":"Sat Feb 20 19:23:25 EST 2021","userID":{"long":...},"fullName":{"string":"Aphrodi\uD83D\uD..."},"tweetID":{"long":...},"tweetSource":{"string":"Twitter for iPhone"},"isTruncated":{"boolean":false},"isRT":{"boolean":false},"tweet":{"string":"RT @deeptrusts: I want someo ..."}}
     
     {"tweetdate":"Sat Feb 20 19:23:25 EST 2021","userID":{"long":...},"fullName":{"string":"Ro ♒\uD83D\uDC96..."},"tweetID":{"long":...},"tweetSource":{"string":"Twitter for iPhone"},"isTruncated":{"boolean":false},"isRT":{"boolean":false},"tweet":{"string":"RT @feelxpain: i fucking fac ..."}}
     
     {"tweetdate":"Sat Feb 20 19:23:25 EST 2021","userID":{"long":...},"fullName":{"string":"nico._.macedo@ni..."},"tweetID":{"long":...},"tweetSource":{"string":"Twitter for Android"},"isTruncated":{"boolean":false},"isRT":{"boolean":false},"tweet":{"string":"@mukti_alin NFR lbinoBateon ..."}}


### Contact
* C. Young: kyang3@lakeheadu.ca
